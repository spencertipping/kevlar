sdocp('sdoc::js::kevlar.db', 'Kevlar database component | Spencer Tipping\nLicensed under the terms of the MIT source code license\n\nIntroduction.\nThis is a fairly straightforward, bomb-proof database designed for easy use, administration, and disaster recovery. All of the contents are stored directly on the filesystem as JSON text\nfiles, and all database operations are stored in a replayable log.\n\nUnlike most databases, kevlar doesn\'t provide general-purpose tables. Instead, you specify the usage pattern for a collection when you create it. Right now there are two kinds of collections:\nassociative hashes and append logs. Associative hashes operate like key-value stores, and append logs operate like text files (though they end up being faster due to record partitioning).\n\nUsing kevlar databases.\nEase of use is an important prerequisite to having something be bulletproof. Here\'s how to create and use a database in kevlar:\n\n| $ node\n  > var k = require(\'./kevlar\');\n  > var test = k.database(\'test\');                              // uses ./test\n  > var foo = test.log(\'foo\');                                  // uses ./test/foo\n  > var bar = test.associative(\'bar\');                          // uses ./test/bar\n\nHere\'s the basic idea of using a log:\n\n| > foo(+new Date() % 3600000, \'hi there\');                     // appends \'hi there\' to the log and uses the current hour as the partition identifier\n  > foo.find(+new Date() % 3600000, function (result) {         // iterates over all records in a single partition\n      console.log(result);\n    });\n  > foo.find(function (result) {                                // iterates over all records in all partitions\n      console.log(result);\n      return false;                                             // stops iteration\n    });\n\nAnd here\'s what it looks like to use an associative table:\n\n| > bar(\'bif\', \'baz\');                                          // associates \'bar\' with \'baz\'\n  > bar.find(\'bif\', function (result) {                         // finds the record associated with \'bif\'; if it doesn\'t exist, your callback is invoked on undefined\n      console.log(result);\n    });\n\nEach of these calls creates any files or directories that don\'t already exist.\n\nIndexing stuff.\nAny self-respecting database will provide a way to maintain lists of objects that have some property. Kevlar is self-respecting by this definition, but only barely. It doesn\'t automatically\nindex things; this is up to you. Here\'s how you build an index (in this example I\'m indexing the first letter of each item in the associative table):\n\n| > var by_letter = test.index(\'by_letter\');                    // create or use an index\n  > by_letter.add(\'b\', \'bar\');                                  // adds \'bar\' to the collection of things starting with b (this operation is idempotent)\n  > by_letter.find(\'b\', function (result) {                     // retrieves everything that starts with b\n      console.log(result);\n      return false;                                             // stops iteration\n    });\n  > by_letter.remove(\'b\', \'bar\');                               // removes \'bar\' from the collection of things starting with b\n\nThe on-disk format for indexes is a bit complex. There\'s a great paper by the Tokutek guys that describes what they call \'fractal trees\', an indexing strategy that is much higher-performance\nthan B-trees on disks where seeks are expensive. I may try to implement those for this database, though it will probably be the simplified version presented in their paper rather than the full\nversion they\'ve released in their TokuDB product.\n\nFor the moment I\'m deferring indexing support. The priority at the moment is to get basic data storage going.\n\n  caterwaul.js_all()(function (exports, require) {\n    exports.database(name) = ensure_directory_sync(name) -returning-\n                             {log: log_generator_for(name), associative: associative_generator_for(name), hourly_log: hourly_log_generator_for(name)},\n\n    where [fs                                   = require(\'fs\'),\n           ensure_directory_sync(name)          = fs.statSync(name) -safely- fs.mkdirSync(name, 0755),\n\n           associative_generator_for(db)(table) = ensure_directory_sync(\'#{db}/#{table}\')\n                                                  -returning- result -effect [it.find(name, f) = result -effect- read_contents(name, f)]\n\n                                                      -where [result(name, value)            = result -effect [set_contents(name, value)],\n\n                                                              djb2_hash(s)                   = bind [h = 5381] in s *![h = (x.charAt(0) * 33 + h) >>> 0] /seq -re- h,\n                                                              prefix_directory_for(name)     = (djb2_hash(name) & 0xfff).toString(36),\n                                                              with_prefix(name, cc)          = fs.stat(\'#{db}/#{table}/#{dir}\', given [err, stat] [\n                                                                                                 err ? fs.mkdir(\'#{db}/#{table}/#{dir}\', 0755, given.nothing in cc(dir)) : cc(dir)])\n                                                                                               -where [dir = prefix_directory_for(name)],\n\n                                                              pending_changes                = {},\n                                                              timeouts                       = {},\n                                                              schedule_commit_for(name)      = timeouts[name] || (timeouts[name] = setTimeout(given.nothing in commit(name), 1000)),\n                                                              commit(name)                   = write_file_contents(name, pending_changes[name]),\n\n                                                              set_contents(name, value)      = pending_changes[name] = value -effect [schedule_commit_for(name)],\n                                                              write_file_contents(name, v)   = with_prefix(name, given.prefix in\n                                                                                                 fs.writeFile(\'#{db}/#{table}/#{prefix}/#{name}\', JSON.stringify(v), \'utf8\',\n                                                                                                              given.err [err /wobbly /when.err, delete pending_changes[name]])),\n\n                                                              read_contents(name, f)         = name in pending_changes ? pending_changes[name] :\n                                                                                               with_prefix(name, given.prefix in\n                                                                                                 fs.readFile(\'#{db}/#{table}/#{prefix}/#{name}\', \'utf8\',\n                                                                                                             given [err, data] [err /wobbly /when.err, f(JSON.parse(data))]))],\n\n           hourly_log_generator_for(db)(table)  = bind [log = log_generator_for(db)(table)] in\n                                                  given [thing] [log(now(), thing)] -effect [it.find(bucket, f) = log.find(bucket, f)]\n                                                                                     -where [now() = \'#{d.getFullYear()}.#{n(d.getMonth() + 1)}#{n(d.getDay())}.#{n(d.getHours())}00\'\n                                                                                                     -where [d = new Date(), n(x) = x < 10 ? \'0#{x}\' : x]],\n           log_generator_for(db)(table)         = ensure_directory_sync(\'#{db}/#{table}\')\n                                                  -returning- result -effect [it.find(bucket, each) = result -effect- read_bucket_contents(bucket, each)]\n\n                                                      -where [result(bucket, stuff)          = result -effect [append_to_bucket(bucket, JSON.stringify(stuff))],\n\n                                                              append_to_bucket(bucket, line) = queue_for(bucket).push(line) -then- schedule_commit_for(bucket),\n\n                                                              timeouts                       = {},\n                                                              schedule_commit_for(bucket)    = timeouts[bucket] || (timeouts[bucket] = setTimeout(given.nothing in commit(bucket), 1000)),\n\n                                                              commit(bucket)                 = write_each_queue_item_for(bucket) -then [delete queues[bucket], delete timeouts[bucket]],\n                                                              write_each_queue_item_for(b)   = write_stream_for(b).end(queues[b].join(\'\\n\') + \'\\n\', \'utf8\'),\n                                                              write_stream_for(bucket)       = fs.createWriteStream(\'#{db}/#{table}/#{bucket}\', {flags: \'a\', mode: 0644}),\n\n                                                              read_bucket_contents(b, f)     = each_line(fs.createReadStream(\'#{db}/#{table}/#{b}\', {encoding: \'utf8\'}), f),\n\n                                                              each_line(stream, f)           = stream -effect [it.on(\'data\', given.piece   in got_pieces(piece.split(/\\n/))),\n                                                                                                               it.on(\'end\',  given.nothing in f(partial) -when- partial.length)]\n\n                                                                                                       -where [partial        = \'\',\n                                                                                                               got_pieces(ps) = f(partial + ps[0])\n                                                                                                                                -then- ps.slice(1, ps.length - 1).forEach(f)\n                                                                                                                                -then [partial = ps.length > 1 && ps[ps.length - 1]]],\n                                                              queues                         = {},\n                                                              queue_for(bucket)              = queues[bucket] || (queues[bucket] = [])]]})(exports, require);');